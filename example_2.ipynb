{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53175b1",
   "metadata": {},
   "source": [
    "# Try python-git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dea7218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 17:29:20,342 - DEBUG - sys.platform='linux', git_executable='git'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo '/home/haianhlt/Documents/bioturing/bioalpha/.git'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from git import Repo\n",
    "\n",
    "repo = Repo(\"/home/haianhlt/Documents/bioturing/bioalpha\")\n",
    "repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17559d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 17:29:20,378 - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=<valid stream>, shell=False, universal_newlines=False)\n",
      "2026-02-02 17:29:20,383 - DEBUG - Popen(['git', 'merge-base', '36223441c04ac7033262998187508e1069c42733', '3e9fa5f0137544480673546b5ee1335d242abe7e'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=None, shell=False, universal_newlines=False)\n",
      "2026-02-02 17:29:20,391 - DEBUG - Popen(['git', 'diff-tree', '3e9fa5f0137544480673546b5ee1335d242abe7e', '36223441c04ac7033262998187508e1069c42733', '-r', '--abbrev=40', '--full-index', '-M', '-p', '--no-ext-diff', '--no-color'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=None, shell=False, universal_newlines=False)\n"
     ]
    }
   ],
   "source": [
    "target = repo.commit(\"tk726_zarr3\")\n",
    "base = repo.commit(\"main\")\n",
    "\n",
    "mutual_base = repo.merge_base(target, base)[0]\n",
    "diffs = mutual_base.diff(target, create_patch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca71ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM src/bioalpha/_version.py TO src/bioalpha/_version.py \n",
      " @@ -28,7 +28,7 @@ version_tuple: VERSION_TUPLE\n",
      " commit_id: COMMIT_ID\n",
      " __commit_id__: COMMIT_ID\n",
      " \n",
      "-__version__ = version = '0.14.3.dev637+g37b41c149.d20250923'\n",
      "-__version_tuple__ = version_tuple = (0, 14, 3, 'dev637', 'g37b41c149.d20250923')\n",
      "+__version__ = version = '0.14.3.dev677+gb337a057a.d20260121'\n",
      "+__version_tuple__ = version_tuple = (0, 14, 3, 'dev677', 'gb337a057a.d20260121')\n",
      " \n",
      "-__commit_id__ = commit_id = 'g37b41c149'\n",
      "+__commit_id__ = commit_id = 'gb337a057a'\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff = diffs[0]\n",
    "print(f\"FROM {diff.a_path} TO {diff.b_path} \\n {diff.diff.decode('utf-8')} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cd03c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM src/bioalpha/store/anndata/__init__.py TO src/bioalpha/store/anndata/__init__.py \n",
      " @@ -25,12 +25,11 @@ from bioalpha.store.anndata.shared import SharedObs, SharedUns, SharedObsm, Shar\n",
      " from bioalpha.constants import IOSpecs, AnnDataAttr, AnnDataAxis, AnnDataViewAttr\n",
      " from bioalpha.utils import (\n",
      "     validate_type,\n",
      "-    get_group_path,\n",
      "     is_group_readonly,\n",
      "     validate_subset_indices,\n",
      "     write_iospec,\n",
      " )\n",
      "-\n",
      "+from bioalpha.compat import get_group_path\n",
      " \n",
      " class BackedRaw:\n",
      "     \"\"\"Currently support read-only\"\"\"\n",
      "@@ -678,7 +677,7 @@ class SharedAssay(Assay):\n",
      "         if not public_read_only:\n",
      "             group = zarr.group(public_path)\n",
      "         else:\n",
      "-            group = zarr.Group(public_path, read_only=public_read_only)\n",
      "+            group = zarr.open_group(public_path, mode = \"r\")\n",
      "         super().__init__(group=group, ref=ref)\n",
      "         self.private = Assay(zarr.open(private_path), ref=ref)\n",
      "         if len(self.private.var_names) == 0:\n",
      "@@ -757,7 +756,7 @@ class SharedAnnData(AppAnnData):\n",
      "         if not public_read_only:\n",
      "             group = zarr.group(public_path)\n",
      "         else:\n",
      "-            group = zarr.Group(public_path, read_only=public_read_only)\n",
      "+            group = zarr.open_group(public_path, mode='r')\n",
      "         super().__init__(group)\n",
      "         self.private = AppAnnData(zarr.open(private_path))\n",
      "         if standardize_path:\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff = diffs[5]\n",
    "print(f\"FROM {diff.a_path} TO {diff.b_path} \\n {diff.diff.decode('utf-8')} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "870a8fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 17:29:20,429 - DEBUG - Popen(['git', 'cat-file', '--batch'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from . import _io_register as _\n",
      "\n",
      "import uuid\n",
      "import zarr\n",
      "import h5py\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy import sparse\n",
      "from functools import cached_property\n",
      "from typing import Optional, Any, Union, Tuple, List, Iterator, Iterable\n",
      "\n",
      "from anndata import AnnData\n",
      "from anndata.utils import make_index_unique\n",
      "from anndata._io.specs import write_elem, read_elem\n",
      "\n",
      "from bioalpha.store._basic import AppCollection\n",
      "from bioalpha.store.matrix import CSCMatrix, CSRMatrix, SpMatrixView, CSCCollection\n",
      "from bioalpha.store.dataframe import BackedObs, BackedVar, AppObs, AppVar, DataFrameView\n",
      "from bioalpha.store.anndata.container import (\n",
      "    BackedObsm, BackedVarm, BackedObsp, BackedVarp, BackedLayers, BackedUns,\n",
      "    AppObsm, AppVarm, AppObsp, AppVarp, AppLayers, AppUns, ContainerView,\n",
      ")\n",
      "from bioalpha.store.anndata.shared import SharedObs, SharedUns, SharedObsm, SharedVarm, SharedVar\n",
      "\n",
      "from bioalpha.constants import IOSpecs, AnnDataAttr, AnnDataAxis, AnnDataViewAttr\n",
      "from bioalpha.utils import (\n",
      "    validate_type,\n",
      "    get_group_path,\n",
      "    is_group_readonly,\n",
      "    validate_subset_indices,\n",
      "    write_iospec,\n",
      ")\n",
      "\n",
      "\n",
      "class BackedRaw:\n",
      "    \"\"\"Currently support read-only\"\"\"\n",
      "    _group: Union[zarr.Group, h5py.File, h5py.Group] = None\n",
      "    _parent = None\n",
      "\n",
      "    def __init__(self, group: Union[zarr.Group, h5py.File, h5py.Group], parent):\n",
      "        self.group = group\n",
      "        self.parent = parent\n",
      "\n",
      "    @property\n",
      "    def _parent_cls(self):\n",
      "        return BackedAnnData\n",
      "\n",
      "    @property\n",
      "    def _var_cls(self):\n",
      "        return BackedVar\n",
      "\n",
      "    def _validate_parent(self, parent):\n",
      "        return validate_type(parent, self._parent_cls)\n",
      "\n",
      "    @property\n",
      "    def group(self):\n",
      "        return self._group\n",
      "\n",
      "    @group.setter\n",
      "    def group(self, group: zarr.Group):\n",
      "        validate_type(group, zarr.Group, h5py.File, h5py.Group)\n",
      "        self._group = group\n",
      "\n",
      "    @property\n",
      "    def parent(self):\n",
      "        return self._parent\n",
      "\n",
      "    @parent.setter\n",
      "    def parent(self, parent):\n",
      "        self._validate_parent(parent)\n",
      "        self._parent = parent\n",
      "\n",
      "    @cached_property\n",
      "    def X(self) -> Union[CSCMatrix, CSRMatrix]:\n",
      "        if AnnDataAttr.X.value in self.group:\n",
      "            return read_elem(self.group[AnnDataAttr.X.value])\n",
      "        return None\n",
      "\n",
      "    @cached_property\n",
      "    def var(self) -> BackedVar:\n",
      "        if AnnDataAttr.VAR.value in self.group:\n",
      "            return self._var_cls(self.group[AnnDataAttr.VAR.value], self)\n",
      "        return None\n",
      "\n",
      "    @cached_property\n",
      "    def var_names(self) -> pd.Index:\n",
      "        if self.var is not None:\n",
      "            return self.var.index\n",
      "        return None\n",
      "\n",
      "\n",
      "class AppRaw(BackedRaw):\n",
      "    @property\n",
      "    def _var_cls(self):\n",
      "        return AppVar\n",
      "\n",
      "\n",
      "class BackedAnnData(AnnData):\n",
      "    \"\"\"Container class for Anndata-like object.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        group: Union[zarr.Group, h5py.File, h5py.Group],\n",
      "        *,\n",
      "        X: Optional[Union[sparse.csc_matrix, np.ndarray]] = None,\n",
      "        obs_names: Iterator[Union[str, np.number]] = None,\n",
      "        var_names: Iterator[Union[str, np.number]] = None,\n",
      "    ):\n",
      "        self.group = group\n",
      "\n",
      "        if obs_names is not None:\n",
      "            self.obs.index = obs_names\n",
      "\n",
      "        if var_names is not None:\n",
      "            self.var.index = var_names\n",
      "\n",
      "        if X is not None:\n",
      "            if AnnDataAttr.X.value in self.group:\n",
      "                raise FileExistsError(\"X is already exists in Zarr Group\")\n",
      "            self.set_X(X)\n",
      "\n",
      "    def create_X(self, matrix_cls: type, overwrite: bool = False):\n",
      "        g = self.group.create_group(AnnDataAttr.X.value, overwrite=overwrite)\n",
      "        return matrix_cls(g, shape=self.shape)\n",
      "\n",
      "    @property\n",
      "    def is_view(self):\n",
      "        return False\n",
      "\n",
      "    @property\n",
      "    def group(self) -> zarr.Group:\n",
      "        return self._group\n",
      "\n",
      "    @group.setter\n",
      "    def group(self, group: zarr.Group):\n",
      "        validate_type(group, zarr.Group, h5py.File, h5py.Group)\n",
      "        self._group = group\n",
      "\n",
      "    @property\n",
      "    def group_path(self) -> zarr.Group:\n",
      "        return get_group_path(self.group)\n",
      "\n",
      "    def set_X(self, X: Union[sparse.csc_matrix, sparse.csr_matrix, SpMatrixView]):\n",
      "        if self.obs is not None:\n",
      "            assert X.shape[0] == len(self.obs)\n",
      "        if self.var is not None:\n",
      "            assert X.shape[1] == len(self.var)\n",
      "        write_elem(self.group, AnnDataAttr.X.value, X)\n",
      "\n",
      "    @property\n",
      "    def _obs_cls(self):\n",
      "        return BackedObs\n",
      "\n",
      "    @property\n",
      "    def _var_cls(self):\n",
      "        return BackedVar\n",
      "\n",
      "    @property\n",
      "    def _obsm_cls(self):\n",
      "        return BackedObsm\n",
      "\n",
      "    @property\n",
      "    def _varm_cls(self):\n",
      "        return BackedVarm\n",
      "\n",
      "    @property\n",
      "    def _obsp_cls(self):\n",
      "        return BackedObsp\n",
      "\n",
      "    @property\n",
      "    def _varp_cls(self):\n",
      "        return BackedVarp\n",
      "\n",
      "    @property\n",
      "    def _layers_cls(self):\n",
      "        return BackedLayers\n",
      "\n",
      "    @property\n",
      "    def _uns_cls(self):\n",
      "        return BackedUns\n",
      "\n",
      "    @property\n",
      "    def _raw_cls(self):\n",
      "        return BackedRaw\n",
      "\n",
      "    def _init_attribute(self, key: str, Attribute, *, ref: Optional[\"BackedAnnData\"] = None):\n",
      "        ref = self if ref is None else ref\n",
      "        if key not in self.group:\n",
      "            if is_group_readonly(self.group):  # HDF5 readonly\n",
      "                return None\n",
      "            self.group.create_group(key)\n",
      "        return Attribute(self.group[key], ref)\n",
      "\n",
      "    @cached_property\n",
      "    def X(self) -> Union[CSCMatrix, CSRMatrix]:\n",
      "        if AnnDataAttr.X.value in self.group:\n",
      "            return read_elem(self.group[AnnDataAttr.X.value])\n",
      "        return None\n",
      "\n",
      "    @cached_property\n",
      "    def obs(self) -> BackedObs:\n",
      "        return self._init_attribute(AnnDataAttr.OBS.value, self._obs_cls)\n",
      "\n",
      "    @cached_property\n",
      "    def var(self) -> BackedVar:\n",
      "        return self._init_attribute(AnnDataAttr.VAR.value, self._var_cls)\n",
      "\n",
      "    @cached_property\n",
      "    def obsm(self) -> BackedObsm:\n",
      "        return self._init_attribute(AnnDataAttr.OBSM.value, self._obsm_cls)\n",
      "\n",
      "    @cached_property\n",
      "    def varm(self) -> BackedVarm:\n",
      "        return self._init_attribute(AnnDataAttr.VARM.value, self._varm_cls)\n",
      "\n",
      "    @cached_property\n",
      "    def obsp(self) -> BackedObsp:\n",
      "        return self._init_attribute(AnnDataAttr.OBSP.value, self._obsp_cls)\n",
      "\n",
      "    @cached_property\n",
      "    def varp(self) -> BackedVarp:\n",
      "        return self._init_attribute(AnnDataAttr.VARP.value, self._varp_cls)\n",
      "\n",
      "    @cached_property\n",
      "    def layers(self) -> BackedLayers:\n",
      "        return self._init_attribute(AnnDataAttr.LAYERS.value, self._layers_cls)\n",
      "\n",
      "    @cached_property\n",
      "    def uns(self) -> BackedUns:\n",
      "        return self._init_attribute(AnnDataAttr.UNS.value, self._uns_cls)\n",
      "\n",
      "    @cached_property\n",
      "    def raw(self) -> BackedRaw:\n",
      "        if AnnDataAttr.RAW.value in self.group:\n",
      "            return self._raw_cls(self.group[AnnDataAttr.RAW.value], self)\n",
      "        return None\n",
      "\n",
      "    @property\n",
      "    def n_obs(self):\n",
      "        return len(self.obs) if self.obs else None\n",
      "\n",
      "    @property\n",
      "    def n_vars(self):\n",
      "        return len(self.var) if self.var else None\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        descr = (\n",
      "            f\"{type(self).__name__} object with \"\n",
      "            f\"n_obs × n_vars = {self.n_obs} × {self.n_vars}\"\n",
      "        )\n",
      "        for attr in AnnDataAttr:\n",
      "            if attr.value not in self.group:\n",
      "                continue\n",
      "            if attr == AnnDataAttr.X:\n",
      "                continue\n",
      "            keys = getattr(self, attr.value).keys()\n",
      "            if len(keys) > 0:\n",
      "                descr += f\"\\n    {attr.value}: {str(list(keys))[1:-1]}\"\n",
      "        return descr\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        super(AnnData, self).__eq__(other)\n",
      "\n",
      "    def __write_attrs(self, f, key, dataset_kwargs):\n",
      "        try:\n",
      "            write_elem(f, key, dict(getattr(self, key)), dataset_kwargs=dataset_kwargs)\n",
      "        except Exception:\n",
      "            write_elem(f, key, {}, dataset_kwargs=dataset_kwargs)\n",
      "\n",
      "    def __write_obs(self, f, *, dataset_kwargs={}):\n",
      "        # Directly use to_df cause issues when the case the .zmetadata bug\n",
      "        # (2 workers saving at the same time)\n",
      "        data = {}\n",
      "        for col in self.obs.columns:\n",
      "            try:\n",
      "                data[col] = self.obs[col]\n",
      "            except Exception:  # pylint: disable=broad-except\n",
      "                continue\n",
      "        df = pd.DataFrame(index=self.obs_names, data=data)\n",
      "        write_elem(f, \"obs\", df, dataset_kwargs=dataset_kwargs)\n",
      "\n",
      "    def __write_group(self, f, *, dataset_kwargs={}):\n",
      "        f.attrs.setdefault(\"encoding-type\", \"anndata\")\n",
      "        f.attrs.setdefault(\"encoding-version\", \"0.1.0\")\n",
      "        if self.raw is not None:\n",
      "            write_elem(f, \"raw\", self.raw, dataset_kwargs=dataset_kwargs)\n",
      "        write_elem(f, \"X\", self.X, dataset_kwargs=dataset_kwargs)\n",
      "        self.__write_obs(f, dataset_kwargs=dataset_kwargs)\n",
      "        write_elem(f, \"var\", self.var.to_df(), dataset_kwargs=dataset_kwargs)\n",
      "        write_elem(f, \"obsm\", self.obsm, dataset_kwargs=dataset_kwargs)\n",
      "        write_elem(f, \"layers\", self.layers, dataset_kwargs=dataset_kwargs)\n",
      "        self.__write_attrs(f, \"varm\", dataset_kwargs)\n",
      "        self.__write_attrs(f, \"obsp\", dataset_kwargs)\n",
      "        self.__write_attrs(f, \"varp\", dataset_kwargs)\n",
      "        write_elem(f, \"uns\", {}, dataset_kwargs=dataset_kwargs)\n",
      "\n",
      "    def write_h5ad(self, filepath, *, dataset_kwargs={}):\n",
      "        with h5py.File(filepath, \"w\") as f:\n",
      "            f = f[\"/\"]\n",
      "            self.__write_group(f, dataset_kwargs=dataset_kwargs)\n",
      "\n",
      "    def __get_mod(self, mod: str):\n",
      "        return self if mod == \"RNA\" else self.assays[mod]\n",
      "\n",
      "    def __write_mapper(self, file, attr: str, assays_list: List[str], kwargs):\n",
      "        dfs = []\n",
      "        for assay in assays_list:\n",
      "            names = getattr(self.__get_mod(assay), attr).index.values\n",
      "            dfs.append(\n",
      "                pd.DataFrame(index=names, data={assay: np.arange(1, len(names) + 1)})\n",
      "            )\n",
      "        dfs = pd.concat(dfs, axis=1).fillna(0)\n",
      "        mapper = {x: dfs[x].astype(int).values for x in assays_list}\n",
      "        write_elem(file, attr, dfs[[]], dataset_kwargs=kwargs)\n",
      "        write_elem(file, f\"{attr}map\", mapper, dataset_kwargs=kwargs)\n",
      "\n",
      "    def write_h5mu(self, filepath, **kwargs):\n",
      "        import mudata\n",
      "        with h5py.File(filepath, \"w\") as file:\n",
      "            file = file[\"/\"]\n",
      "            assay_list = [\"RNA\", *list(self.assays.keys())]\n",
      "            self.__write_mapper(file, \"obs\", assay_list, kwargs)\n",
      "            self.__write_mapper(file, \"var\", assay_list, kwargs)\n",
      "\n",
      "            attrs = file.attrs\n",
      "            attrs[\"axis\"] = 0\n",
      "\n",
      "            mod = file.require_group(\"mod\")\n",
      "            for k in assay_list:\n",
      "                group = mod.require_group(k)\n",
      "                adata = self.__get_mod(k)\n",
      "                adata.__write_group(group, dataset_kwargs=kwargs)\n",
      "                attrs = group.attrs\n",
      "                attrs[\"encoder\"] = \"mudata\"\n",
      "                attrs[\"encoder-version\"] = mudata.__version__\n",
      "\n",
      "            mod_attrs = mod.attrs\n",
      "            mod_attrs[\"mod-order\"] = assay_list\n",
      "\n",
      "            attrs = file.attrs\n",
      "            attrs[\"encoding-type\"] = \"MuData\"\n",
      "            attrs[\"encoding-version\"] = mudata.__mudataversion__\n",
      "            attrs[\"encoder\"] = \"mudata\"\n",
      "            attrs[\"encoder-version\"] = mudata.__version__\n",
      "\n",
      "\n",
      "class AppAnnData(BackedAnnData):\n",
      "    @property\n",
      "    def _obs_cls(self):\n",
      "        return AppObs\n",
      "\n",
      "    @property\n",
      "    def _var_cls(self):\n",
      "        return AppVar\n",
      "\n",
      "    @property\n",
      "    def _obsm_cls(self):\n",
      "        return AppObsm\n",
      "\n",
      "    @property\n",
      "    def _varm_cls(self):\n",
      "        return AppVarm\n",
      "\n",
      "    @property\n",
      "    def _obsp_cls(self):\n",
      "        return AppObsp\n",
      "\n",
      "    @property\n",
      "    def _varp_cls(self):\n",
      "        return AppVarp\n",
      "\n",
      "    @property\n",
      "    def _layers_cls(self):\n",
      "        return AppLayers\n",
      "\n",
      "    @property\n",
      "    def _uns_cls(self):\n",
      "        return AppUns\n",
      "\n",
      "    @property\n",
      "    def _raw_cls(self):\n",
      "        return AppRaw\n",
      "\n",
      "    @cached_property\n",
      "    def subclusters(self):\n",
      "        return self._init_attribute(AnnDataAttr.SUBCLUSTERS.value, SubClusters)\n",
      "\n",
      "    @cached_property\n",
      "    def assays(self):\n",
      "        return self._init_attribute(AnnDataAttr.ASSAYS.value, Assays)\n",
      "\n",
      "\n",
      "class AnnDataView(AppAnnData):\n",
      "    def __init__(\n",
      "        self,\n",
      "        group: zarr.Group,\n",
      "        ref: AppAnnData,\n",
      "        *,\n",
      "        indices: Tuple[Optional[np.ndarray], Optional[np.ndarray]] = None,\n",
      "    ):\n",
      "        self._ref = ref\n",
      "        self.group = group\n",
      "\n",
      "        if not is_group_readonly(group):\n",
      "            write_iospec(IOSpecs.SUBCLUSTER.value, group, overwrite=False)\n",
      "\n",
      "        if indices is not None:\n",
      "            self.init_indices(indices)\n",
      "\n",
      "    @property\n",
      "    def n_obs(self):\n",
      "        obs_indices = self.indices[AnnDataAxis.OBS.value]\n",
      "        return self.ref.n_obs if obs_indices is None else len(obs_indices)\n",
      "\n",
      "    @property\n",
      "    def n_vars(self):\n",
      "        vars_indices = self.indices[AnnDataAxis.VAR.value]\n",
      "        return self.ref.n_vars if vars_indices is None else len(vars_indices)\n",
      "\n",
      "    @property\n",
      "    def ref(self):\n",
      "        return self._ref\n",
      "\n",
      "    @cached_property\n",
      "    def indices(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
      "        obs_indices = read_elem(self.group[AnnDataViewAttr.OBS_INDICES.value]) \\\n",
      "            if AnnDataViewAttr.OBS_INDICES.value in self.group else None\n",
      "        var_indices = read_elem(self.group[AnnDataViewAttr.VAR_INDICES.value]) \\\n",
      "            if AnnDataViewAttr.VAR_INDICES.value in self.group else None\n",
      "        return (obs_indices, var_indices)\n",
      "\n",
      "    def init_indices(self, indices: Tuple[Optional[np.ndarray], Optional[np.ndarray]]):\n",
      "        if indices[AnnDataAxis.OBS.value] is not None:\n",
      "            validate_subset_indices(indices[AnnDataAxis.OBS.value], self.ref.n_obs)\n",
      "            write_elem(self.group, AnnDataViewAttr.OBS_INDICES.value,\n",
      "                       indices[AnnDataAxis.OBS.value])\n",
      "        if indices[AnnDataAxis.VAR.value] is not None:\n",
      "            validate_subset_indices(indices[AnnDataAxis.VAR.value], self.ref.n_vars)\n",
      "            write_elem(self.group, AnnDataViewAttr.VAR_INDICES.value,\n",
      "                       indices[AnnDataAxis.VAR.value])\n",
      "\n",
      "    @cached_property\n",
      "    def X(self) -> SpMatrixView:\n",
      "        return SpMatrixView(self.ref.X, self.indices)\n",
      "\n",
      "    @cached_property\n",
      "    def obs(self) -> DataFrameView:\n",
      "        return DataFrameView(self.ref.obs, self)\n",
      "\n",
      "    @cached_property\n",
      "    def var(self) -> DataFrameView:\n",
      "        return DataFrameView(self.ref.var, self)\n",
      "\n",
      "    @cached_property\n",
      "    def layers(self) -> ContainerView:\n",
      "        return ContainerView(self.ref.layers, self)\n",
      "\n",
      "    @cached_property\n",
      "    def assays(self):\n",
      "        return self.ref._init_attribute(AnnDataAttr.ASSAYS.value, Assays, ref=self)\n",
      "\n",
      "    @property\n",
      "    def subclusters(self):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def is_view(self):\n",
      "        return True\n",
      "\n",
      "\n",
      "class SubClusters(AppCollection):\n",
      "    def __init__(self, group: zarr.Group, parent) -> None:\n",
      "        self._group = group\n",
      "        self._parent = parent\n",
      "\n",
      "        if not is_group_readonly(group):\n",
      "            write_iospec(IOSpecs.BTR_DICT.value, group, overwrite=False)\n",
      "\n",
      "    @property\n",
      "    def parent(self):\n",
      "        return self._parent\n",
      "\n",
      "    @property\n",
      "    def group(self) -> zarr.Group:\n",
      "        return self._group\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.keys())\n",
      "\n",
      "    def _validate_value(self, values: Any):\n",
      "        pass\n",
      "\n",
      "    def _get_item_impl(self, key: str) -> pd.Series:\n",
      "        return AnnDataView(self.group[key], self.parent)\n",
      "\n",
      "    def _set_item_impl(self, key, values):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def _del_item_impl(self, key: str):\n",
      "        if len(key) == 0:\n",
      "            raise KeyError(\"Unable to delete item with the empty key.\")\n",
      "        if key in self.group:\n",
      "            del self.group[key]\n",
      "\n",
      "    def create_subcluster(self, key: str, indices: Tuple[np.ndarray, np.ndarray]):\n",
      "        ret = AnnDataView(self.group.create_group(\n",
      "            key, overwrite=True), self.parent, indices=indices)\n",
      "        self._add_name(key, key)\n",
      "        return ret\n",
      "\n",
      "\n",
      "class Assay(AppAnnData):\n",
      "    def __init__(\n",
      "        self,\n",
      "        group: zarr.Group,\n",
      "        ref: Union[AppAnnData, AnnDataView],\n",
      "        X: Optional[Union[sparse.csc_matrix, np.ndarray]] = None,\n",
      "        var_names: Iterator[Union[str, np.number]] = None,\n",
      "    ):\n",
      "        self.ref = ref\n",
      "        self.group = group\n",
      "\n",
      "        if not is_group_readonly(group):\n",
      "            write_iospec(IOSpecs.ASSAY.value, group, overwrite=False)\n",
      "\n",
      "        if var_names is not None:\n",
      "            self.var.index = var_names\n",
      "\n",
      "        if X is not None:\n",
      "            if AnnDataAttr.X.value in self.group:\n",
      "                raise FileExistsError(\"X is already exists in Zarr Group\")\n",
      "            self.set_X(X)\n",
      "\n",
      "    @property\n",
      "    def ref(self):\n",
      "        return self._ref\n",
      "\n",
      "    @ref.setter\n",
      "    def ref(self, val: Union[AppAnnData, AnnDataView]):\n",
      "        if val.is_view and val.indices[AnnDataAxis.VAR.value] is not None:\n",
      "            raise RuntimeError(\"Var view on assays is not supported\")\n",
      "        self._ref = val\n",
      "\n",
      "    @property\n",
      "    def n_obs(self):\n",
      "        return self.ref.n_obs\n",
      "\n",
      "    @cached_property\n",
      "    def obs(self) -> BackedObs:\n",
      "        return self.ref.obs\n",
      "\n",
      "    @cached_property\n",
      "    def obsm(self) -> BackedObsm:\n",
      "        return self.ref.obsm\n",
      "\n",
      "    @cached_property\n",
      "    def obsp(self) -> BackedObsp:\n",
      "        return self.ref.obsp\n",
      "\n",
      "    @cached_property\n",
      "    def uns(self) -> BackedUns:\n",
      "        return self.ref.uns\n",
      "\n",
      "    @cached_property\n",
      "    def X(self) -> SpMatrixView:\n",
      "        if AnnDataAttr.X.value in self.group:\n",
      "            ret = read_elem(self.group[AnnDataAttr.X.value])\n",
      "            return SpMatrixView(ret, self.ref.indices) if self.ref.is_view else ret\n",
      "        return None\n",
      "\n",
      "    @property\n",
      "    def subclusters(self):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def assays(self):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def is_view(self):\n",
      "        return self.ref.is_view\n",
      "\n",
      "    @property\n",
      "    def layers(self):\n",
      "        attr = super().layers\n",
      "        if self.is_view:\n",
      "            return ContainerView(attr, self.ref)\n",
      "        return attr\n",
      "\n",
      "\n",
      "class Assays(AppCollection):\n",
      "    def __init__(self, group: zarr.Group, parent) -> None:\n",
      "        self._group = group\n",
      "        self._parent = parent\n",
      "\n",
      "        if not is_group_readonly(group):\n",
      "            write_iospec(IOSpecs.BTR_DICT.value, group, overwrite=False)\n",
      "\n",
      "    @property\n",
      "    def parent(self):\n",
      "        return self._parent\n",
      "\n",
      "    @property\n",
      "    def group(self) -> zarr.Group:\n",
      "        return self._group\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.keys())\n",
      "\n",
      "    def _validate_value(self, values: Any):\n",
      "        pass\n",
      "\n",
      "    def _get_item_impl(self, key: str) -> pd.Series:\n",
      "        return Assay(self.group[key], self.parent)\n",
      "\n",
      "    def _set_item_impl(self, key, values):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def _del_item_impl(self, key: str):\n",
      "        if len(key) == 0:\n",
      "            raise KeyError(\"Unable to delete item with the empty key.\")\n",
      "        if key in self.group:\n",
      "            del self.group[key]\n",
      "\n",
      "    def create_assay(self, key: str, var_names: List[str], X: Optional[Union[sparse.csc_matrix, np.ndarray]] = None):\n",
      "        elem_id = self.generate_elem_id()\n",
      "        group = self.group.create_group(elem_id, overwrite=True)\n",
      "        ret = Assay(group, self.parent, X=X, var_names=var_names)\n",
      "        old_uid = self._add_name(elem_id, key)\n",
      "        if old_uid is not None and old_uid in self.group:\n",
      "            del self.group[old_uid]\n",
      "        return ret\n",
      "\n",
      "\n",
      "def create_anndata_collection(\n",
      "    group: zarr.Group, refs: List[AppAnnData], *,\n",
      "    barcode_prefixes: Optional[List[str]] = None,\n",
      "):\n",
      "    if AnnDataAttr.X.value in group:\n",
      "        raise FileExistsError(\"X is already exists in Zarr Group\")\n",
      "\n",
      "    if barcode_prefixes is None:\n",
      "        obs_names = np.concatenate([adata.obs_names.values for adata in refs])\n",
      "    else:\n",
      "        assert len(barcode_prefixes) == len(refs), \\\n",
      "            \"Found mismatch length of prefix barcodes and reference anndata\"\n",
      "        obs_names = np.concatenate([\n",
      "            pd.Index([prefix] * adata.n_obs).astype(str).str.cat(adata.obs_names.values).values\n",
      "            for prefix, adata in zip(barcode_prefixes, refs)\n",
      "        ])\n",
      "    obs_names = pd.Index(obs_names, dtype=str)\n",
      "\n",
      "    if len(set(obs_names)) < len(obs_names):\n",
      "        obs_names = make_index_unique(obs_names, join=\"-btr-\")\n",
      "\n",
      "    var_names = np.concatenate([adata.var_names.values for adata in refs])\n",
      "    var_names = pd.Index(np.unique(var_names))\n",
      "\n",
      "    ref_indices = [pd.Index(adata.var_names).get_indexer(var_names) for adata in refs]\n",
      "    sparse_matrices = [adata.X for adata in refs]\n",
      "\n",
      "    return AppAnnData(\n",
      "        group,\n",
      "        X=CSCCollection(refs=sparse_matrices, ref_indices=ref_indices),\n",
      "        obs_names=obs_names, var_names=var_names,\n",
      "    )\n",
      "\n",
      "\n",
      "class SharedAssay(Assay):\n",
      "    def __init__(\n",
      "            self,\n",
      "            private_path: str,\n",
      "            public_path: str,\n",
      "            ref: \"SharedAnnData\",\n",
      "            var_names: Iterable[str],\n",
      "            public_read_only: bool = True\n",
      "    ):\n",
      "        if not public_read_only:\n",
      "            group = zarr.group(public_path)\n",
      "        else:\n",
      "            group = zarr.Group(public_path, read_only=public_read_only)\n",
      "        super().__init__(group=group, ref=ref)\n",
      "        self.private = Assay(zarr.open(private_path), ref=ref)\n",
      "        if len(self.private.var_names) == 0:\n",
      "            self.private = Assay(zarr.open(private_path), ref=ref, var_names=var_names)\n",
      "        self.ref = ref\n",
      "\n",
      "    def _init_private_attribute(self, key: str, SharedAttribute, Attribute):\n",
      "        if key not in self.private.group:\n",
      "            self.private.group.create_group(key)\n",
      "        return SharedAttribute(self.private.group[key], self, self._init_attribute(key, Attribute))\n",
      "\n",
      "    @cached_property\n",
      "    def var(self) -> SharedVar:\n",
      "        return self._init_private_attribute(AnnDataAttr.VAR.value, SharedVar, AppVar)\n",
      "\n",
      "    @cached_property\n",
      "    def varm(self) -> SharedVarm:\n",
      "        return self._init_private_attribute(AnnDataAttr.VARM.value, SharedVarm, AppVarm)\n",
      "\n",
      "\n",
      "class SharedAssays(Assays):\n",
      "    def __init__(self, zarr_group: zarr.Group, parent: \"SharedAnnData\", reference: Assays):\n",
      "        super().__init__(zarr_group, parent)\n",
      "        self.reference: Assays = reference\n",
      "        self._name2id = self.reference._name2id\n",
      "\n",
      "    def __contains__(self, key):\n",
      "        return super().__contains__(key) or self.reference.__contains__(key)\n",
      "\n",
      "    def _get_item_impl(self, key: str):\n",
      "        if key in self.reference.group.keys():\n",
      "            if key not in self.group.keys():\n",
      "                self.group.create_group(key)\n",
      "        return SharedAssay(\n",
      "            private_path=super()._get_item_impl(key).group_path,\n",
      "            public_path=self.reference[key].group_path,\n",
      "            ref=self.parent,\n",
      "            var_names=self.reference[key].var_names,\n",
      "            public_read_only=self.parent._public_read_only\n",
      "        )\n",
      "\n",
      "\n",
      "class SharedSubClusters(SubClusters):\n",
      "    def __init__(self, zarr_group: zarr.Group, parent, reference):\n",
      "        super().__init__(zarr_group, parent)\n",
      "        self.reference: SubClusters = reference\n",
      "\n",
      "    def __contains__(self, key):\n",
      "        if self.reference:\n",
      "            return super().__contains__(key) or self.reference.__contains__(key)\n",
      "        return super().__contains__(key)\n",
      "\n",
      "    def _get_item_impl(self, key: str):\n",
      "        if key in self.group:\n",
      "            return super()._get_item_impl(key)\n",
      "        if self.reference:\n",
      "            return self.reference[key]\n",
      "        raise KeyError(f\"Subcluster {key} not found in both private and reference data.\")\n",
      "\n",
      "    def keys(self):\n",
      "        # If the origin data haven't create subcluster yet, the reference will return None.\n",
      "        # See _init_attribute for more infomation.\n",
      "        if self.reference:\n",
      "            return set([*super().keys(), *self.reference.keys()])\n",
      "        return super().keys()\n",
      "\n",
      "\n",
      "class SharedAnnData(AppAnnData):\n",
      "    def __init__(\n",
      "        self,\n",
      "        private_path: str,\n",
      "        public_path: str,\n",
      "        public_read_only: bool = True,\n",
      "        standardize_path: Optional[str] = None,\n",
      "    ):\n",
      "        if not public_read_only:\n",
      "            group = zarr.group(public_path)\n",
      "        else:\n",
      "            group = zarr.Group(public_path, read_only=public_read_only)\n",
      "        super().__init__(group)\n",
      "        self.private = AppAnnData(zarr.open(private_path))\n",
      "        if standardize_path:\n",
      "            self.std = AppAnnData(zarr.open(standardize_path))\n",
      "            self.std._init_attribute(AnnDataAttr.OBS.value, AppObs)\n",
      "        else:\n",
      "            self.std = None\n",
      "\n",
      "        self._public_read_only = public_read_only\n",
      "\n",
      "    def _init_private_attribute(self, key: str):\n",
      "        if key not in self.private.group:\n",
      "            self.private.group.create_group(key)\n",
      "\n",
      "    def _init_shared_attribute(self, key: str, SharedAttribute, Attribute):\n",
      "        self._init_private_attribute(key)\n",
      "        return SharedAttribute(self.private.group[key], self, self._init_attribute(key, Attribute))\n",
      "\n",
      "    @cached_property\n",
      "    def obs(self) -> SharedObs:\n",
      "        if self.std:\n",
      "            self._init_private_attribute(AnnDataAttr.OBS.value)\n",
      "            return SharedObs(\n",
      "                zarr_group=self.private.group[AnnDataAttr.OBS.value],\n",
      "                parent=self,\n",
      "                reference=self._init_attribute(AnnDataAttr.OBS.value, AppObs),\n",
      "                std=self.std.obs,\n",
      "            )\n",
      "        return self._init_shared_attribute(AnnDataAttr.OBS.value, SharedObs, AppObs)\n",
      "\n",
      "    @cached_property\n",
      "    def obsm(self) -> SharedObsm:\n",
      "        return self._init_shared_attribute(AnnDataAttr.OBSM.value, SharedObsm, AppObsm)\n",
      "\n",
      "    @cached_property\n",
      "    def assays(self) -> SharedAssays:\n",
      "        return self._init_shared_attribute(AnnDataAttr.ASSAYS.value, SharedAssays, Assays)\n",
      "\n",
      "    @cached_property\n",
      "    def subclusters(self):\n",
      "        return self._init_shared_attribute(\n",
      "            AnnDataAttr.SUBCLUSTERS.value, SharedSubClusters, SubClusters\n",
      "        )\n",
      "\n",
      "    @cached_property\n",
      "    def uns(self) -> SharedUns:\n",
      "        return self._init_shared_attribute(AnnDataAttr.UNS.value, SharedUns, AppUns)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blob = mutual_base.tree[diff.a_path]\n",
    "content = blob.data_stream.read().decode('utf-8')\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34078892",
   "metadata": {},
   "source": [
    "# Try tree sitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92b99580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter_languages import get_language, get_parser\n",
    "\n",
    "language = get_language('python')\n",
    "parser = get_parser(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04e55b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tree_sitter.Tree object at 0x7b75ed211530>\n"
     ]
    }
   ],
   "source": [
    "tree = parser.parse(content.encode('utf-8'))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a215cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ -25,12 +25,11 @@ from bioalpha.store.anndata.shared import SharedObs, SharedUns, SharedObsm, Shar\n",
      " from bioalpha.constants import IOSpecs, AnnDataAttr, AnnDataAxis, AnnDataViewAttr\n",
      " from bioalpha.utils import (\n",
      "     validate_type,\n",
      "-    get_group_path,\n",
      "     is_group_readonly,\n",
      "     validate_subset_indices,\n",
      "     write_iospec,\n",
      " )\n",
      "-\n",
      "+from bioalpha.compat import get_group_path\n",
      " \n",
      " class BackedRaw:\n",
      "     \"\"\"Currently support read-only\"\"\"\n",
      "@@ -678,7 +677,7 @@ class SharedAssay(Assay):\n",
      "         if not public_read_only:\n",
      "             group = zarr.group(public_path)\n",
      "         else:\n",
      "-            group = zarr.Group(public_path, read_only=public_read_only)\n",
      "+            group = zarr.open_group(public_path, mode = \"r\")\n",
      "         super().__init__(group=group, ref=ref)\n",
      "         self.private = Assay(zarr.open(private_path), ref=ref)\n",
      "         if len(self.private.var_names) == 0:\n",
      "@@ -757,7 +756,7 @@ class SharedAnnData(AppAnnData):\n",
      "         if not public_read_only:\n",
      "             group = zarr.group(public_path)\n",
      "         else:\n",
      "-            group = zarr.Group(public_path, read_only=public_read_only)\n",
      "+            group = zarr.open_group(public_path, mode='r')\n",
      "         super().__init__(group)\n",
      "         self.private = AppAnnData(zarr.open(private_path))\n",
      "         if standardize_path:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(diff.diff.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acfd6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path_to_deepest_node_at_line(node, line):\n",
    "    if not (node.start_point[0] <= line <= node.end_point[0]):\n",
    "        return None\n",
    "\n",
    "    for child in node.children:\n",
    "        child_path = find_path_to_deepest_node_at_line(child, line)\n",
    "        if child_path:\n",
    "            return [node] + child_path\n",
    "\n",
    "    return [node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8642d88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Node type=import_from_statement, start_point=(0, 0), end_point=(0, 31)>,\n",
       " <Node type=import_statement, start_point=(2, 0), end_point=(2, 11)>,\n",
       " <Node type=import_statement, start_point=(3, 0), end_point=(3, 11)>,\n",
       " <Node type=import_statement, start_point=(4, 0), end_point=(4, 11)>,\n",
       " <Node type=import_statement, start_point=(5, 0), end_point=(5, 18)>,\n",
       " <Node type=import_statement, start_point=(6, 0), end_point=(6, 19)>,\n",
       " <Node type=import_from_statement, start_point=(7, 0), end_point=(7, 24)>,\n",
       " <Node type=import_from_statement, start_point=(8, 0), end_point=(8, 37)>,\n",
       " <Node type=import_from_statement, start_point=(9, 0), end_point=(9, 72)>,\n",
       " <Node type=import_from_statement, start_point=(11, 0), end_point=(11, 27)>,\n",
       " <Node type=import_from_statement, start_point=(12, 0), end_point=(12, 43)>,\n",
       " <Node type=import_from_statement, start_point=(13, 0), end_point=(13, 51)>,\n",
       " <Node type=import_from_statement, start_point=(15, 0), end_point=(15, 47)>,\n",
       " <Node type=import_from_statement, start_point=(16, 0), end_point=(16, 83)>,\n",
       " <Node type=import_from_statement, start_point=(17, 0), end_point=(17, 88)>,\n",
       " <Node type=import_from_statement, start_point=(18, 0), end_point=(21, 1)>,\n",
       " <Node type=import_from_statement, start_point=(22, 0), end_point=(22, 97)>,\n",
       " <Node type=import_from_statement, start_point=(24, 0), end_point=(24, 81)>,\n",
       " <Node type=import_from_statement, start_point=(25, 0), end_point=(31, 1)>,\n",
       " <Node type=class_definition, start_point=(34, 0), end_point=(88, 19)>,\n",
       " <Node type=class_definition, start_point=(91, 0), end_point=(94, 21)>,\n",
       " <Node type=class_definition, start_point=(97, 0), end_point=(343, 57)>,\n",
       " <Node type=class_definition, start_point=(346, 0), end_point=(389, 69)>,\n",
       " <Node type=class_definition, start_point=(392, 0), end_point=(467, 19)>,\n",
       " <Node type=class_definition, start_point=(470, 0), end_point=(508, 18)>,\n",
       " <Node type=class_definition, start_point=(511, 0), end_point=(587, 19)>,\n",
       " <Node type=class_definition, start_point=(590, 0), end_point=(631, 18)>,\n",
       " <Node type=function_definition, start_point=(634, 0), end_point=(665, 5)>,\n",
       " <Node type=class_definition, start_point=(668, 0), end_point=(698, 88)>,\n",
       " <Node type=class_definition, start_point=(701, 0), end_point=(720, 9)>,\n",
       " <Node type=class_definition, start_point=(723, 0), end_point=(745, 29)>,\n",
       " <Node type=class_definition, start_point=(748, 0), end_point=(806, 84)>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.root_node.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45532c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Node type=import_from_statement, start_point=(13, 0), end_point=(13, 51)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.root_node.children[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49b848dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Node type=module, start_point=(0, 0), end_point=(807, 0)>,\n",
       " <Node type=class_definition, start_point=(34, 0), end_point=(88, 19)>,\n",
       " <Node type=block, start_point=(35, 4), end_point=(88, 19)>,\n",
       " <Node type=decorated_definition, start_point=(58, 4), end_point=(61, 27)>,\n",
       " <Node type=function_definition, start_point=(59, 4), end_point=(61, 27)>,\n",
       " <Node type=block, start_point=(60, 8), end_point=(61, 27)>,\n",
       " <Node type=expression_statement, start_point=(60, 8), end_point=(60, 63)>,\n",
       " <Node type=call, start_point=(60, 8), end_point=(60, 63)>,\n",
       " <Node type=identifier, start_point=(60, 8), end_point=(60, 21)>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = find_path_to_deepest_node_at_line(tree.root_node, 60)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7a27797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'module'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.root_node.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ef909",
   "metadata": {},
   "source": [
    "# Call Diff Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85931160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.diff.diff_analysis import DiffAnalyzer\n",
    "\n",
    "diff_analysier = DiffAnalyzer(\"/home/haianhlt/Documents/bioturing/bioalpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c5e9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 17:29:20,555 - DEBUG - sys.platform='linux', git_executable='git'\n",
      "2026-02-02 17:29:20,558 - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=<valid stream>, shell=False, universal_newlines=False)\n",
      "2026-02-02 17:29:20,563 - DEBUG - Popen(['git', 'merge-base', '36223441c04ac7033262998187508e1069c42733', '3e9fa5f0137544480673546b5ee1335d242abe7e'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=None, shell=False, universal_newlines=False)\n",
      "2026-02-02 17:29:20,570 - DEBUG - Popen(['git', 'diff-tree', '3e9fa5f0137544480673546b5ee1335d242abe7e', '36223441c04ac7033262998187508e1069c42733', '-r', '--abbrev=40', '--full-index', '-M', '--raw', '-z', '--no-color'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=None, shell=False, universal_newlines=False)\n",
      "2026-02-02 17:29:20,585 - DEBUG - Popen(['git', 'diff-tree', '3e9fa5f0137544480673546b5ee1335d242abe7e', '36223441c04ac7033262998187508e1069c42733', '-r', '--abbrev=40', '--full-index', '-M', '-p', '--no-ext-diff', '--no-color'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=None, shell=False, universal_newlines=False)\n",
      "2026-02-02 17:29:20,610 - DEBUG - Popen(['git', 'cat-file', '--batch'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<src.diff.diff_analysis.KudoDiff at 0x7b75cbce7850>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75ed0ee5d0>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb34c10>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbd24350>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb2d610>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbd13f90>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbd13990>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbcf2d90>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75edafd290>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb58250>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb5a750>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb58a90>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb5a790>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb5a510>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbcfad50>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbd11750>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb57c50>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbd12610>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb2e850>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbb362d0>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75ed10aad0>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c65fe610>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c65fee50>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c6534f10>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75edb25910>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c65b6b90>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75edb49a90>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c653b490>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c653b390>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c65be410>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c65bca90>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75ed10ab50>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c65bfb90>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c658b450>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c64009d0>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c6400890>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75cbd24550>,\n",
       " <src.diff.diff_analysis.KudoDiff at 0x7b75c65ea9d0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = diff_analysier.analyze_diffs(\"tk726_zarr3\", \"main\")\n",
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dcf714d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.semantic_ast.ast_file_analysis.SemanticASTNode at 0x7b75cbb50590>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sast = diffs[2].semantic_ast.root\n",
    "sast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3664121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Node type=import_statement, start_point=(0, 0), end_point=(0, 9)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sast.children[0].ast_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4a716fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== src/bioalpha/store/dataframe.py === \n",
      "<Node type=module, start_point=(0, 0), end_point=(405, 0)>\n",
      ".....\n",
      "from abc import ABC, abstractmethod\n",
      ".....\n",
      "class OneDim(ABC):\n",
      "    \"\"\"\n",
      "    Validation helper mapping keys to one-dimensional array-like structures\n",
      "    aligned with an axis of the parent AnnData-like object.\n",
      "    This is compatible with AnnData `obs` and `var`.\n",
      "    \"\"\"\n",
      "\n",
      "    _parent: Any\n",
      "\n",
      "    def __init__(self, parent: Any):\n",
      "        self._parent = parent\n",
      "\n",
      "    @property\n",
      "    def parent(self):\n",
      "        return self._parent\n",
      "\n",
      "    @property\n",
      "    @abstractmethod\n",
      "    def axis(self) -> int:\n",
      "        \"\"\"Indicates the axis being represented (0 for `obs`, 1 for `var`).\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def _get_index_impl(self) -> object:\n",
      "        \"\"\"Retrieves index of the object.\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def _set_index_impl(self, index: Iterator[Union[str, np.number]]):\n",
      "        \"\"\"Sets index of the object.\"\"\"\n",
      "\n",
      "    @property\n",
      "    @abstractmethod\n",
      "    def dtypes(self) -> pd.Series:\n",
      "        \"\"\"Returns the data types of the elements, similar to pandas.DataFrame.dtypes.\"\"\"\n",
      "\n",
      "    @property\n",
      "    @abstractmethod\n",
      "    def columns(self) -> List[str]:\n",
      "        \"\"\"Returns the names of the columns, similar to pandas.DataFrame.columns.\"\"\"\n",
      "\n",
      "    @property\n",
      "    def index(self) -> Iterator[Union[str, np.number]]:\n",
      "        \"\"\"Returns the index of the obs or var.\"\"\"\n",
      "        return self._get_index_impl()\n",
      "\n",
      "    @index.setter\n",
      "    def index(self, index: Iterator[Union[str, np.number]]):\n",
      "        \"\"\"Sets a new index for the obs or var.\"\"\"\n",
      "        if self.parent.shape[self.axis]:\n",
      "            assert len(index) == self.parent.shape[self.axis]\n",
      "        return self._set_index_impl(index)\n",
      "\n",
      "    @abstractmethod\n",
      "    def __len__(self) -> int:\n",
      "        pass\n",
      "\n",
      "    def _validate_value(self, values):\n",
      "        if len(values) != len(self):\n",
      "            raise ValueError(\n",
      "                f\"Length of values ({len(values)}) \"\n",
      "                f\"does not match length of index ({len(self)})\"\n",
      "            )\n",
      "        if isinstance(values, np.ndarray):\n",
      "            if values.ndim > 1:\n",
      "                raise ValueError(\n",
      "                    f\"Expected a 1D array, got an array with shape {values.shape}\"\n",
      "                )\n",
      "\n",
      "    # The AxisArraysBase.to_df is too slow and required a lot of memory.\n",
      "    def to_df(self) -> pd.DataFrame:\n",
      "        \"\"\"Convert to pandas DataFrame\"\"\"\n",
      "        keys = self.columns\n",
      "        if keys:\n",
      "            return pd.DataFrame({key: self[key] for key in keys}, index=self.index)\n",
      "        return pd.DataFrame(index=self.index)\n",
      "\n",
      "    def copy(self):\n",
      "        return self.to_df().copy()\n",
      "\n",
      "    @property\n",
      "    def dim_names(self) -> pd.Index:\n",
      "        return self.index\n",
      ".....\n",
      "class BackedDataFrame(ABC):\n",
      "    _index_key: str\n",
      "\n",
      "    def __init__(self, group: Union[zarr.Group, h5py.Group]) -> None:\n",
      "        self._group = group\n",
      "        if not is_group_readonly(group):\n",
      "            write_iospec(IOSpecs.DATAFRAME.value, group, overwrite=False)\n",
      "        self._index_key = group.attrs.get(\n",
      "            DataFrameAttr.INDEX_KEY.value, DataFrameAttr.INDEX_KEY.value)\n",
      "\n",
      "    @property\n",
      "    def group(self) -> Union[zarr.Group, h5py.Group]:\n",
      "        return self._group\n",
      "\n",
      "    @property\n",
      "    def columns(self) -> List[str]:\n",
      "        return [col for col in self.keys() if col != self._index_key]\n",
      "\n",
      "    @property\n",
      "    def dtypes(self) -> pd.Series:\n",
      "        dtypes = {}\n",
      "        for key in self.columns:\n",
      "            try:\n",
      "                group_key = self.group[key]\n",
      "            except Exception:\n",
      "                group_key = self.group[self._name2id[key]]\n",
      "            arr_type = dict(group_key.attrs).get(IOSpecsAttr.ENCODING_TYPE.value, \"array\")\n",
      "            dtypes[key] = (\n",
      "                np.dtype(\"float32\") if arr_type == \"array\"\n",
      "                else pd.CategoricalDtype(categories=group_key[\"categories\"])\n",
      "            )\n",
      "        return pd.Series(dtypes)\n",
      "\n",
      "    def _validate_key(self, key: str):\n",
      "        if key == self._index_key:\n",
      "            raise KeyError(f\"Unable to manipulate index: `{self._index_key}`\")\n",
      "        if \"/\" in key:\n",
      "            raise ValueError(\n",
      "                \"Key should not contain '/', it will break when store in .hdf5 or .zarr file.\")\n",
      "\n",
      "    def _get_item_impl(self, key: Union[str, List[str]]) -> pd.Series:\n",
      "        if isinstance(key, list):\n",
      "            return pd.DataFrame({k: self[k] for k in key}, index=self.index)\n",
      "\n",
      "        element = self.group[key]\n",
      "\n",
      "        # deprecated\n",
      "        if element.attrs.get(\"metadata_type\", \"\") == \"category\":\n",
      "            catagory_names = element.attrs[\"category_names\"]\n",
      "            return pd.Series(np.array(catagory_names)[element[:]], name=key, index=self.index)\n",
      "\n",
      "        return pd.Series(read_elem(element), name=key, index=self.index)\n",
      "\n",
      "    def _set_item_impl(self, key: str, values: Union[pd.Series, pd.Categorical, np.ndarray]):\n",
      "        if key == self._index_key:\n",
      "            raise KeyError(f\"Unable to manipulate index: `{self._index_key}`\")\n",
      "        write_elem(self.group, key, convert_df_series(values))\n",
      "\n",
      "    def _del_item_impl(self, key: str):\n",
      "        if key == self._index_key:\n",
      "            raise KeyError(f\"Unable to delete index: `{self._index_key}`\")\n",
      "        if len(key) == 0:\n",
      "            raise KeyError(\"Unable to delete item with the empty key.\")\n",
      "        if key in self.group:\n",
      "            del self.group[key]\n",
      "\n",
      "    def _get_index_impl(self) -> pd.Series:\n",
      "        if self._index_key not in self.group:\n",
      "            return []\n",
      "        return pd.Series(read_elem(self.group[self._index_key]), name=self._index_key)\n",
      "\n",
      "    def _set_index_impl(self, index: Iterator[Union[str, np.number]]):\n",
      "        index = pd.Series(index)\n",
      "        if self._index_key in self.group:\n",
      "            raise KeyError(f\"Index already exists: {self._index_key}\")\n",
      "        write_elem(self.group, self._index_key, index.values)\n",
      "\n",
      "    def __len__(self):\n",
      "        if self._index_key not in self.group:\n",
      "            return 0\n",
      "        return len(self.group[self._index_key])\n",
      ".....\n",
      "class DataFrameView:\n",
      "    _parent: Any\n",
      "\n",
      "    def __init__(self, ref: BackedDataFrame, parent: Any) -> None:\n",
      "        self.ref = ref\n",
      "        self._parent = parent\n",
      "\n",
      "    @property\n",
      "    def parent(self) -> Any:\n",
      "        return self._parent\n",
      "\n",
      "    @property\n",
      "    def ref(self) -> BackedDataFrame:\n",
      "        return self._ref\n",
      "\n",
      "    @ref.setter\n",
      "    def ref(self, ref: BackedDataFrame):\n",
      "        validate_type(ref, BackedDataFrame)\n",
      "        self._ref = ref\n",
      "\n",
      "    @property\n",
      "    def axis(self) -> int:\n",
      "        return self.ref.axis\n",
      "\n",
      "    def keys(self) -> List[str]:\n",
      "        return self.ref.keys()\n",
      "\n",
      "    @property\n",
      "    def columns(self) -> List[str]:\n",
      "        return self.ref.columns\n",
      "\n",
      "    @property\n",
      "    def dtypes(self) -> pd.Series:\n",
      "        return self.ref.dtypes\n",
      "\n",
      "    def __len__(self) -> int:\n",
      "        return self.parent.shape[self.axis]\n",
      "\n",
      "    def __getitem__(self, key) -> pd.Series:\n",
      "        indices = self.parent.indices[self.axis]\n",
      "        if indices is None:\n",
      "            return self.ref[key]\n",
      "        return self.ref[key].iloc[indices]\n",
      "\n",
      "    @property\n",
      "    def index(self) -> Iterator[Union[str, np.number]]:\n",
      "        ref_index = self.ref.index\n",
      "        if self.parent.indices[self.axis] is None:\n",
      "            return ref_index\n",
      "        return ref_index.iloc[self.parent.indices[self.axis]]\n",
      "\n",
      "    def to_df(self) -> pd.DataFrame:\n",
      "        \"\"\"Convert to pandas DataFrame\"\"\"\n",
      "        keys = [key for key in self.keys() if key != DataFrameAttr.INDEX_KEY.value]\n",
      "        if keys:\n",
      "            return pd.concat([self[key] for key in keys], axis=1, keys=keys)\n",
      "        return pd.DataFrame(index=self.index)\n",
      "\n",
      "    def copy(self):\n",
      "        return self.to_df().copy()\n",
      "\n",
      "    def __iter__(self):\n",
      "        yield from self.columns\n",
      "\n",
      "    def read_attrs(self, key: str):\n",
      "        return self.ref.read_attrs(key)\n",
      "\n",
      "    @property\n",
      "    def is_view(self):\n",
      "        return True\n",
      "\n",
      "    @property\n",
      "    def dim_names(self) -> pd.Index:\n",
      "        return self.index\n",
      "\n",
      "    def get_id_by_name(self, name):\n",
      "        return self.ref.get_id_by_name(name)\n",
      ".....\n",
      "class AppObs(BackedDataFrame, OneDim, AppCollection):\n",
      "    def __init__(self, group: Union[zarr.Group, h5py.Group], parent):\n",
      "        BackedDataFrame.__init__(self, group)\n",
      "        OneDim.__init__(self, parent)\n",
      "\n",
      "    @property\n",
      "    def axis(self):\n",
      "        return AnnDataAxis.OBS.value\n",
      "\n",
      "    @cached_property\n",
      "    def z_metadata_group(self) -> Union[Attributes, Dict[str, Dict[str, str]]]:\n",
      "        return Attributes(\n",
      "            self.group.store,\n",
      "            key=os.path.join(self.group_path, MetadataGroup.FILE_NAME.value)\n",
      "        )\n",
      "\n",
      "    # overload AppCollection.__setitem__() to automatically save changed metadata id\n",
      "    def __setitem__(self, key, value):\n",
      "        old_id = self.get_id_by_name(key)\n",
      "        AppCollection.__setitem__(self, key, value)\n",
      "        new_id = self.get_id_by_name(key)\n",
      "\n",
      "        if old_id in self.z_metadata_group.get(MetadataGroup.MAPPING.value, {}) and old_id != new_id:\n",
      "            self._update_metadata_group(\n",
      "                MetadataGroup.MAPPING.value,\n",
      "                new_id,\n",
      "                self._pop_metadata_group(MetadataGroup.MAPPING.value, old_id)\n",
      "            )\n",
      "\n",
      "    def _update_metadata_group(self, field: str, key: str, value: str):\n",
      "        if field not in [MetadataGroup.MAPPING.value, MetadataGroup.GROUPS.value]:\n",
      "            raise KeyError(f\"Key {field} is not supported in metadata groups.\")\n",
      "        tmp = self.z_metadata_group.get(field, {})\n",
      "        tmp[key] = value\n",
      "        self.z_metadata_group[field] = tmp\n",
      "\n",
      "    def _pop_metadata_group(self, field: str, key: str):\n",
      "        if field not in [MetadataGroup.MAPPING.value, MetadataGroup.GROUPS.value]:\n",
      "            raise KeyError(f\"Key {field} is not supported in metadata groups.\")\n",
      "        tmp = self.z_metadata_group.get(field, {})\n",
      "        ret = tmp.pop(key)\n",
      "        self.z_metadata_group[field] = tmp\n",
      "        return ret\n",
      "\n",
      "    def create_metadata_group(self, name: str):\n",
      "        if name in self.z_metadata_group.get(MetadataGroup.GROUPS.value, {}):\n",
      "            raise KeyError(f\"Metadata group {name} already exists.\")\n",
      "\n",
      "        self._update_metadata_group(MetadataGroup.GROUPS.value, name, self.generate_elem_id())\n",
      "        return self.z_metadata_group.asdict()\n",
      "\n",
      "    def rename_metadata_group(self, old_name: str, new_name: str):\n",
      "        if old_name not in self.z_metadata_group.get(MetadataGroup.GROUPS.value, {}):\n",
      "            raise KeyError(f\"Metadata group {old_name} does not exist.\")\n",
      "        if new_name in self.z_metadata_group.get(MetadataGroup.GROUPS.value, {}):\n",
      "            raise KeyError(f\"Metadata group {new_name} already exists.\")\n",
      "\n",
      "        self._update_metadata_group(\n",
      "            MetadataGroup.GROUPS.value,\n",
      "            new_name,\n",
      "            self._pop_metadata_group(MetadataGroup.GROUPS.value, old_name)\n",
      "        )\n",
      "        return self.z_metadata_group.asdict()\n",
      "\n",
      "    def delete_metadata_group(self, group: str):\n",
      "        if group not in self.z_metadata_group.get(MetadataGroup.GROUPS.value, {}):\n",
      "            raise KeyError(f\"Metadata group {group} does not exist.\")\n",
      "\n",
      "        self._group_id_2_name = {\n",
      "            uuid: name\n",
      "            for name, uuid in self.z_metadata_group[MetadataGroup.GROUPS.value].items()\n",
      "        }\n",
      "        for m_uuid, g_uuid in list(self.z_metadata_group.get(MetadataGroup.MAPPING.value, {}).items()):\n",
      "            if self._group_id_2_name[g_uuid] == group:\n",
      "                self._pop_metadata_group(MetadataGroup.MAPPING.value, m_uuid)\n",
      "\n",
      "        self._pop_metadata_group(MetadataGroup.GROUPS.value, group)\n",
      "        return self.z_metadata_group.asdict()\n",
      "\n",
      "    def delete_metadata_from_current_group(self, metadata: str):\n",
      "        self._pop_metadata_group(MetadataGroup.MAPPING.value, self._name2id[metadata])\n",
      "        return self.z_metadata_group.asdict()\n",
      "\n",
      "    def change_metadata_group(self, group: str, metadata: str):\n",
      "        if group not in self.z_metadata_group.get(MetadataGroup.GROUPS.value, {}):\n",
      "            raise KeyError(f\"Metadata group {group} does not exists.\")\n",
      "\n",
      "        self._update_metadata_group(\n",
      "            MetadataGroup.MAPPING.value,\n",
      "            self._name2id[metadata],\n",
      "            self.z_metadata_group[MetadataGroup.GROUPS.value][group],\n",
      "        )\n",
      "        return self.z_metadata_group.asdict()\n",
      "\n",
      "    def change_multiple_metadata_group(self, groups: Union[str, List[str]], metadatas: List[str]):\n",
      "        if isinstance(groups, list):\n",
      "            if len(groups) != len(metadatas):\n",
      "                raise KeyError(f\"Length of group and metadata must be the same.\")\n",
      "        else:\n",
      "            groups = [groups] * len(metadatas)\n",
      "\n",
      "        for g, m in zip(groups, metadatas):\n",
      "            self.change_metadata_group(g, m)\n",
      "\n",
      "        return self.z_metadata_group.asdict()\n",
      "\n",
      "    def delete_multiple_metadata_from_groups(self, metadatas: List[str]):\n",
      "        for m in metadatas:\n",
      "            self._pop_metadata_group(MetadataGroup.MAPPING.value, self._name2id[m])\n",
      "\n",
      "        return self.z_metadata_group.asdict()\n",
      ".....\n"
     ]
    }
   ],
   "source": [
    "print(diffs[7].get_source_code_context())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c474eb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ -25,12 +25,11 @@ from bioalpha.store.anndata.shared import SharedObs, SharedUns, SharedObsm, Shar\n",
      " from bioalpha.constants import IOSpecs, AnnDataAttr, AnnDataAxis, AnnDataViewAttr\n",
      " from bioalpha.utils import (\n",
      "     validate_type,\n",
      "-    get_group_path,\n",
      "     is_group_readonly,\n",
      "     validate_subset_indices,\n",
      "     write_iospec,\n",
      " )\n",
      "-\n",
      "+from bioalpha.compat import get_group_path\n",
      " \n",
      " class BackedRaw:\n",
      "     \"\"\"Currently support read-only\"\"\"\n",
      "@@ -678,7 +677,7 @@ class SharedAssay(Assay):\n",
      "         if not public_read_only:\n",
      "             group = zarr.group(public_path)\n",
      "         else:\n",
      "-            group = zarr.Group(public_path, read_only=public_read_only)\n",
      "+            group = zarr.open_group(public_path, mode = \"r\")\n",
      "         super().__init__(group=group, ref=ref)\n",
      "         self.private = Assay(zarr.open(private_path), ref=ref)\n",
      "         if len(self.private.var_names) == 0:\n",
      "@@ -757,7 +756,7 @@ class SharedAnnData(AppAnnData):\n",
      "         if not public_read_only:\n",
      "             group = zarr.group(public_path)\n",
      "         else:\n",
      "-            group = zarr.Group(public_path, read_only=public_read_only)\n",
      "+            group = zarr.open_group(public_path, mode='r')\n",
      "         super().__init__(group)\n",
      "         self.private = AppAnnData(zarr.open(private_path))\n",
      "         if standardize_path:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(diffs[5].diff_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261afd7",
   "metadata": {},
   "source": [
    "# Try phase 1 - light review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 13:25:40,819 - DEBUG - sys.platform='linux', git_executable='git'\n",
      "2026-02-03 13:25:40,821 - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=<valid stream>, shell=False, universal_newlines=False)\n",
      "2026-02-03 13:25:40,826 - DEBUG - Popen(['git', 'merge-base', '36223441c04ac7033262998187508e1069c42733', '3e9fa5f0137544480673546b5ee1335d242abe7e'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=None, shell=False, universal_newlines=False)\n",
      "2026-02-03 13:25:40,831 - DEBUG - Popen(['git', 'diff-tree', '3e9fa5f0137544480673546b5ee1335d242abe7e', '36223441c04ac7033262998187508e1069c42733', '-r', '--abbrev=40', '--full-index', '-M', '--raw', '-z', '--no-color'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=None, shell=False, universal_newlines=False)\n",
      "2026-02-03 13:25:40,848 - DEBUG - Popen(['git', 'diff-tree', '3e9fa5f0137544480673546b5ee1335d242abe7e', '36223441c04ac7033262998187508e1069c42733', '-r', '--abbrev=40', '--full-index', '-M', '-p', '--no-ext-diff', '--no-color'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=None, shell=False, universal_newlines=False)\n",
      "2026-02-03 13:25:40,871 - DEBUG - Popen(['git', 'cat-file', '--batch'], cwd=/home/haianhlt/Documents/bioturing/bioalpha, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    }
   ],
   "source": [
    "from src.llm_client import DiffLightReviewer\n",
    "\n",
    "reviewer = DiffLightReviewer(\n",
    "    \"GOOGLE_API_KEY\", \n",
    "    \"gemini-2.5-flash\", \n",
    "    repo_path=\"/home/haianhlt/Documents/bioturing/bioalpha\",\n",
    "    base_branch=\"main\",\n",
    "    target_branch=\"tk726_zarr3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32b5168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 13:25:43,210 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 13:25:43,214 - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None\n",
      "2026-02-03 13:25:43,348 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7ec5e5dc2350>\n",
      "2026-02-03 13:25:43,349 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7ec5e5d29e20> server_hostname='generativelanguage.googleapis.com' timeout=None\n",
      "2026-02-03 13:25:46,142 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7ec5e5dc8f10>\n",
      "2026-02-03 13:25:46,144 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2026-02-03 13:25:46,145 - DEBUG - send_request_headers.complete\n",
      "2026-02-03 13:25:46,146 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2026-02-03 13:25:46,229 - DEBUG - send_request_body.complete\n",
      "2026-02-03 13:25:46,230 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2026-02-03 13:26:41,654 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Tue, 03 Feb 2026 06:26:41 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=55400'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "2026-02-03 13:26:41,658 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 13:26:41,660 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2026-02-03 13:26:41,665 - DEBUG - receive_response_body.complete\n",
      "2026-02-03 13:26:41,667 - DEBUG - response_closed.started\n",
      "2026-02-03 13:26:41,669 - DEBUG - response_closed.complete\n",
      "2026-02-03 13:26:41,677 - DEBUG - close.started\n",
      "2026-02-03 13:26:41,679 - DEBUG - close.complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"state\": \"CONTINUE\",\\n  \"confidence\": 0.9,\\n  \"request_review_funcs\": [\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"zarr_copy\",\\n      \"reason\": \"This function re-implements Zarr\\'s `copy` for 1D arrays, specifically for Zarr v3 compatibility. This is a critical and potentially complex piece of logic involving chunking and direct data manipulation, which could have performance and correctness implications. Needs thorough validation across various Zarr store types and edge cases.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"open_metadata_attrs\",\\n      \"reason\": \"This function handles `ZarrAttributes` initialization differently for Zarr v2 and v3. For Zarr v3, it creates a group if the key doesn\\'t exist. This implicit group creation for attributes needs careful consideration regarding intended behavior, especially in concurrent write scenarios or if the \\'key\\' might sometimes refer to an array rather than a group holding attributes.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"get_group_path\",\\n      \"reason\": \"This function centralizes the logic for obtaining paths from various Zarr/H5 group and array types, differentiating between Zarr v2 and v3. Its correctness is crucial for all file I/O operations throughout the codebase that interact with Zarr/H5 groups. Needs robust testing with diverse data structures and storage backends.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"write_shapes\",\\n      \"reason\": \"This function is a new compatibility layer for writing shapes, seemingly wrapping or replacing `spatialdata._io.io_shapes.write_shapes`. Its internal logic, particularly how it interacts with `group.store_path.store.root` for parquet file paths and `element_format` versions, is sensitive to Zarr version changes and `spatialdata` internals. Thorough review is needed to ensure correct behavior and data integrity.\"\\n    },\\n    {\\n      \"file\": \"src/pyproject.toml\",\\n      \"function\": \"(module)\",\\n      \"reason\": \"The dependency updates, particularly `numpy>=2.0.0` and `anndata==0.13.0.dev0+gb796d59fe.d20260119`, are significant and could introduce breaking changes or require extensive code adjustments to align with the new versions. The `requires-python = \\'>=3.10\\'` also breaks compatibility with Python 3.8/3.9. This constitutes a major version bump effort and requires comprehensive regression testing.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\\n      \"function\": \"SpatialDataPoints._set_item_impl\",\\n      \"reason\": \"The change from `write_points(..., group=self.group, name=key)` to creating a subgroup `dst_group = self.group.create_group(key); write_points(..., group=dst_group)` implies a structural change in how point data is stored. This might impact existing data structures, loading mechanisms, and compatibility with external tools expecting a flat structure or different nested hierarchy for points. A migration strategy or backward compatibility implications need to be considered.\"\\n    }\\n  ],\\n  \"quick_review\": [\\n    {\\n      \"file\": \"src/bioalpha/_version.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated version string and tuple to \\'0.14.3.dev677+gb337a057a.d20260121\\' and corresponding commit ID. This is a standard version bump.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"is_anndata_v12\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Introduced a simple helper function to check if the `anndata` version is 0.12.0 or newer based on `packaging.version.Version`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"is_zarr_v3\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Introduced a simple helper function to check if the `zarr` version is 3.0.0 or newer based on `packaging.version.Version`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"get_reader\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"This function provides an AnnData version-aware reader retrieval mechanism, using `Reader(_REGISTRY)` for AnnData >= 0.12.0. This is a crucial compatibility shim.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"consolidate_metadata\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Wrapped `zarr.consolidate_metadata` to handle Zarr v3 local stores using `group.store.root` vs `group.store.path` for other cases.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/compat.py\",\\n      \"function\": \"create_dataset\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Provides a compatibility layer for creating datasets, using `group.create_dataset` (for Zarr v2/h5py) or `group.create_array` (for Zarr v3) based on exception handling. This is a common pattern for API divergence.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/core/preprocessing/_transform_merge_matrices.py\",\\n      \"function\": \"transform_merge_matrices\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Replaced `os.path.join(dst_group.store.path, dst_group.path)` with `get_group_path(dst_group)`. This correctly delegates path resolution to the new compatibility layer.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/core/preprocessing/_transform_merge_matrices.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Removed `import os` as its functionality for path joining has been abstracted into `bioalpha.compat.get_group_path`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/core/stats/_thresholding.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`, reflecting the module refactoring.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/_basic.py\",\\n      \"function\": \"_set_latest_update\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Replaced direct `zarr.consolidate_metadata(os.path.join(self.group.store.path, self.group.path))` with the new `consolidate_metadata(self.group)` from `bioalpha.compat`, improving maintainability across Zarr versions.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/_basic.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated imports for `get_group_path` and `consolidate_metadata` to point to the new `bioalpha.compat` module.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/_basic.py\",\\n      \"function\": \"_validate_key\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Formatted the long string literal onto multiple lines for improved readability.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/anndata/__init__.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/anndata/__init__.py\",\\n      \"function\": \"SharedAssay.__init__\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed `zarr.Group(public_path, read_only=public_read_only)` to `zarr.open_group(public_path, mode=\\'r\\')`. This aligns with the Zarr v3 API for read-only group access.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/anndata/__init__.py\",\\n      \"function\": \"SharedAnnData.__init__\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed `zarr.Group(public_path, read_only=public_read_only)` to `zarr.open_group(public_path, mode=\\'r\\')`. Similar Zarr v3 API compatibility update for read-only access.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/anndata/container.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/dataframe.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Removed direct `os`, `uuid`, and `zarr.attrs` imports. Added `ZarrAttributes` and `open_metadata_attrs` from `bioalpha.compat`, and `get_df_column_length` from `bioalpha.utils`, streamlining dependencies.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/dataframe.py\",\\n      \"function\": \"BackedDataFrame.__len__\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Replaced `len(self.group[self._index_key])` with `get_df_column_length(self.group[self._index_key])`. This provides a more robust way to determine the length of Zarr/H5 dataset columns, especially for categorical data stored with \\'codes\\'.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/dataframe.py\",\\n      \"function\": \"AppObs.z_metadata_group\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Delegated Zarr attribute group creation to `open_metadata_attrs` from `bioalpha.compat`, ensuring compatibility with different Zarr versions for metadata handling.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/__init__.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Changed elements in `__all__` list from unquoted identifiers to quoted strings, which is a stylistic improvement for clarity and consistency.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_base.py\",\\n      \"function\": \"ChunkReader.get_chunk\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Changed `len(self._array)` to `self._array.shape[0]`. This is a more robust way to access the size of the first dimension for array-like objects, improving reliability.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated imports for `get_group_path`, `zarr_copy`, `get_reader`, and `create_dataset` to source them from `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\\n      \"function\": \"_patched_methods\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Modified the AnnData reader retrieval to use `get_reader(_REGISTRY, f, iospec)` from `bioalpha.compat`, ensuring version-aware handling of AnnData readers.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\\n      \"function\": \"write_sparse_collection\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Replaced `group.create_dataset(...)` with `create_dataset(group, ...)` from `bioalpha.compat`, ensuring compatibility for dataset creation across Zarr versions.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\\n      \"function\": \"read_csc_collection\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed `CSCMatrix(zarr.Group(path, read_only=True))` to `CSCMatrix(zarr.open_group(path, mode=\\'r\\'))`, aligning with Zarr v3 API for read-only group access.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\\n      \"function\": \"read_csr_collection\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed `CSCMatrix(zarr.Group(path, read_only=True))` to `CSCMatrix(zarr.open_group(path, mode=\\'r\\'))`, aligning with Zarr v3 API for read-only group access for CSR collections (note: the code creates a `CSCMatrix` for CSR collection, which might be a copy-paste error or intentional and needs clarification).\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated imports for `get_group_path`, `zarr_copy`, and `create_dataset` from `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\\n      \"function\": \"ActualSpMatrix.set_indptr\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Replaced `self.group.create_dataset(...)` with `create_dataset(self.group, ...)` from `bioalpha.compat`, abstracting Zarr v2/v3 dataset creation.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\\n      \"function\": \"ActualSpMatrix.init_indices\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Replaced `self.group.create_dataset(...)` with `create_dataset(self.group, ...)` from `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\\n      \"function\": \"ActualSpMatrix.set_indices\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed type comparison from `is np.dtype` to `== np.dtype` for `val.dtype`, improving correctness. Also, calls `zarr_copy` from `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\\n      \"function\": \"ActualSpMatrix.init_data\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Replaced `self.group.create_dataset(...)` with `create_dataset(self.group, ...)` from `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\\n      \"function\": \"ActualSpMatrix.set_data\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed type comparison from `is np.dtype` to `== np.dtype` for `val.dtype`, improving correctness. Also, calls `zarr_copy` from `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\\n      \"function\": \"ActualSpMatrix._from_diff_major_axis\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Uses `get_group_path` from `bioalpha.compat` for temporary directory creation, and added parentheses around the conditional `batch.tocsr() if ... else batch.tocsc()` for clarity.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/_base.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/_io_register.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/images.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Removed `zarr.attrs` import. Updated `get_group_path` from `bioalpha.utils` to `bioalpha.compat`. Added `create_dataset` and `is_zarr_v3` from `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/images.py\",\\n      \"function\": \"SpatialDataImages.set_channel_histogram\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Replaced `group.create_dataset(...)` with `create_dataset(group, ...)` from `bioalpha.compat`, ensuring Zarr v2/v3 compatibility.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/images.py\",\\n      \"function\": \"SpatialDataImages.generator_writer\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Replaced `group.create_dataset(...)` with `create_dataset(group, ...)` for Zarr v2/v3 compatibility. `dimension_separator` is now passed via `zarr_write_kwargs` (if Zarr v3), indicating more flexible Zarr array creation.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/images.py\",\\n      \"function\": \"SpatialDataImages.generator_writer\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Explicitly casts `level_shape` elements to `int` using `tuple(int(x) for x in ...)` to prevent potential float values from `np.ceil` in shapes.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/images.py\",\\n      \"function\": \"SpatialDataImages._write.register(zarr.Array)\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Added conditional logic for handling `chunk_key_encoding` (Zarr v3) vs `dimension_separator` (Zarr v2) when optimizing Zarr writing, ensuring compatibility with new Zarr features.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Updated imports for `get_group_path` and `write_shapes` to come from `bioalpha.compat`, reflecting the compatibility module.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\\n      \"function\": \"BackedGeoDF.__init__\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed argument name `format` to `element_format` in the `write_shapes` call. This is an API change in `write_shapes` and needs to be consistent with the wrapped/modified function in `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\\n      \"function\": \"BackedGeoDF.geometry.setter\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed argument name `format` to `element_format` in the `write_shapes` call, similar to the `__init__` method.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\\n      \"function\": \"SpatialDataShapes.read\",\\n      \"severity\": \"medium\",\\n      \"comment\": \"Changed `read_elem(self.group[element])` to `_read_shapes(get_group_path(self.group[element]))`. This shifts from using `anndata`\\'s generic element reader to `spatialdata`\\'s specific shape reader via the compatibility `get_group_path`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/utils.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"high\",\\n      \"comment\": \"Removed `os`, `Path`, `zarr.storage` imports, and `get_group_path`, `zarr_copy` functions. These have been moved to `bioalpha/compat.py`. This is a significant refactoring and requires verifying all former call sites now import from `bioalpha.compat`.\"\\n    },\\n    {\\n      \"file\": \"src/bioalpha/utils.py\",\\n      \"function\": \"get_df_column_length\",\\n      \"severity\": \"low\",\\n      \"comment\": \"New utility function to correctly determine the length of a DataFrame column, accounting for AnnData-style categorical storage (which uses a \\'codes\\' array within a group) or direct Zarr/H5 datasets.\"\\n    },\\n    {\\n      \"file\": \"test/spatial/test_imaging.py\",\\n      \"function\": \"(module)\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Imported `create_dataset` from `bioalpha.compat` to use the new compatibility function for creating Zarr datasets in tests.\"\\n    },\\n    {\\n      \"file\": \"test/spatial/test_imaging.py\",\\n      \"function\": \"test_otsu_thresholding\",\\n      \"severity\": \"low\",\\n      \"comment\": \"Replaced `group.create_dataset(...)` with `create_dataset(group, ...)` from `bioalpha.compat` in the test setup, ensuring consistency with the new Zarr compatibility layer.\"\\n    }\\n  ]\\n}\\n```'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_response = reviewer.llm_review()\n",
    "raw_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b85ea2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== src/bioalpha/core/preprocessing/_transform_merge_matrices.py === \n",
      "<Node type=module, start_point=(0, 0), end_point=(105, 0)>\n",
      ".....\n",
      "import os\n",
      ".....\n",
      "import numba as nb\n",
      ".....\n",
      "from anndata._io.specs import write_elem, read_elem\n",
      ".....\n",
      "def transform_merge_matrices(\n",
      "    input_batches: List[BatchData], dst: Union[zarr.Group, CSCMatrix],\n",
      "    transform_fn: Callable[[int, int, int, sparse.csr_matrix], sparse.csr_matrix],\n",
      "    n_cols: int, dtype: np.dtype = None,\n",
      ") -> int:\n",
      "    n_rows = 0\n",
      "    dst_group = dst if isinstance(dst, zarr.Group) else dst.group\n",
      "    dst_path = os.path.join(dst_group.store.path, dst_group.path)\n",
      "\n",
      "    data_dtype, index_dtype = _get_matrix_dtype(input_batches)\n",
      "    data_dtype = data_dtype if dtype is None else dtype\n",
      "\n",
      "    with tempfile.TemporaryDirectory(dir=dst_path, suffix=\".temp\") as tempdir:\n",
      "        temp_group = zarr.open(tempdir)\n",
      "        batches: List[Tuple[int, CSCMatrix]] = []\n",
      "        indptr = np.zeros(n_cols + 1, dtype=index_dtype)\n",
      "\n",
      "        for i, input_batch in enumerate(input_batches):\n",
      "            for s, e, batch_mtx in batch_loader(input_batch.csr_mtx, batch_sz=settings.get_batch_sz()):\n",
      "                batch_mtx = _map_features(batch_mtx, n_cols, input_batch.cols_mapping)\n",
      "                batch_mtx = transform_fn(i, s, e, batch_mtx)\n",
      "                batch_mtx = batch_mtx.tocsc()\n",
      "                indptr[1:] += np.diff(batch_mtx.indptr)\n",
      "\n",
      "                batch_name = f\"{i}.{s}_{e}.zarr\"\n",
      "                write_elem(temp_group, batch_name, batch_mtx)\n",
      "                batches.append((n_rows, read_elem(temp_group[batch_name])))\n",
      "                n_rows += batch_mtx.shape[0]\n",
      "\n",
      "                # free memory\n",
      "                del batch_mtx\n",
      "\n",
      "        dst_mtx = CSCMatrix(dst_group, shape=(n_rows, n_cols)\n",
      "                            ) if isinstance(dst, zarr.Group) else dst\n",
      "        indptr = np.cumsum(indptr, out=indptr)\n",
      "        dst_mtx.set_indptr(indptr)\n",
      "        dst_mtx.init_data(data_dtype)\n",
      "        dst_mtx.init_indices(index_dtype)\n",
      "\n",
      "        for s, e in batch_loader(dst_mtx, batch_sz=settings.get_batch_sz() // 2, load_data=False):\n",
      "            batch = sparse.vstack([submtx[:, s:e] for _, submtx in batches])\n",
      "            batch_s, batch_e = dst_mtx.indptr[s], dst_mtx.indptr[e]\n",
      "            dst_mtx.data[batch_s:batch_e] = batch.data\n",
      "            dst_mtx.indices[batch_s:batch_e] = batch.indices\n",
      "\n",
      "            # free memory\n",
      "            del batch\n",
      "\n",
      "    return n_rows\n",
      ".....\n"
     ]
    }
   ],
   "source": [
    "print(reviewer.kudo_diffs[2].get_source_code_context())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68fec7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"state\": \"CONTINUE\",\n",
      "    \"confidence\": 0.9,\n",
      "    \"request_review_funcs\": [\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"zarr_copy\",\n",
      "            \"reason\": \"This function re-implements Zarr's `copy` for 1D arrays, specifically for Zarr v3 compatibility. This is a critical and potentially complex piece of logic involving chunking and direct data manipulation, which could have performance and correctness implications. Needs thorough validation across various Zarr store types and edge cases.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"open_metadata_attrs\",\n",
      "            \"reason\": \"This function handles `ZarrAttributes` initialization differently for Zarr v2 and v3. For Zarr v3, it creates a group if the key doesn't exist. This implicit group creation for attributes needs careful consideration regarding intended behavior, especially in concurrent write scenarios or if the 'key' might sometimes refer to an array rather than a group holding attributes.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"get_group_path\",\n",
      "            \"reason\": \"This function centralizes the logic for obtaining paths from various Zarr/H5 group and array types, differentiating between Zarr v2 and v3. Its correctness is crucial for all file I/O operations throughout the codebase that interact with Zarr/H5 groups. Needs robust testing with diverse data structures and storage backends.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"write_shapes\",\n",
      "            \"reason\": \"This function is a new compatibility layer for writing shapes, seemingly wrapping or replacing `spatialdata._io.io_shapes.write_shapes`. Its internal logic, particularly how it interacts with `group.store_path.store.root` for parquet file paths and `element_format` versions, is sensitive to Zarr version changes and `spatialdata` internals. Thorough review is needed to ensure correct behavior and data integrity.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/pyproject.toml\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"reason\": \"The dependency updates, particularly `numpy>=2.0.0` and `anndata==0.13.0.dev0+gb796d59fe.d20260119`, are significant and could introduce breaking changes or require extensive code adjustments to align with the new versions. The `requires-python = '>=3.10'` also breaks compatibility with Python 3.8/3.9. This constitutes a major version bump effort and requires comprehensive regression testing.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\n",
      "            \"function\": \"SpatialDataPoints._set_item_impl\",\n",
      "            \"reason\": \"The change from `write_points(..., group=self.group, name=key)` to creating a subgroup `dst_group = self.group.create_group(key); write_points(..., group=dst_group)` implies a structural change in how point data is stored. This might impact existing data structures, loading mechanisms, and compatibility with external tools expecting a flat structure or different nested hierarchy for points. A migration strategy or backward compatibility implications need to be considered.\"\n",
      "        }\n",
      "    ],\n",
      "    \"quick_review\": [\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/_version.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated version string and tuple to '0.14.3.dev677+gb337a057a.d20260121' and corresponding commit ID. This is a standard version bump.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"is_anndata_v12\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Introduced a simple helper function to check if the `anndata` version is 0.12.0 or newer based on `packaging.version.Version`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"is_zarr_v3\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Introduced a simple helper function to check if the `zarr` version is 3.0.0 or newer based on `packaging.version.Version`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"get_reader\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"This function provides an AnnData version-aware reader retrieval mechanism, using `Reader(_REGISTRY)` for AnnData >= 0.12.0. This is a crucial compatibility shim.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"consolidate_metadata\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Wrapped `zarr.consolidate_metadata` to handle Zarr v3 local stores using `group.store.root` vs `group.store.path` for other cases.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/compat.py\",\n",
      "            \"function\": \"create_dataset\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Provides a compatibility layer for creating datasets, using `group.create_dataset` (for Zarr v2/h5py) or `group.create_array` (for Zarr v3) based on exception handling. This is a common pattern for API divergence.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/core/preprocessing/_transform_merge_matrices.py\",\n",
      "            \"function\": \"transform_merge_matrices\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Replaced `os.path.join(dst_group.store.path, dst_group.path)` with `get_group_path(dst_group)`. This correctly delegates path resolution to the new compatibility layer.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/core/preprocessing/_transform_merge_matrices.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Removed `import os` as its functionality for path joining has been abstracted into `bioalpha.compat.get_group_path`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/core/stats/_thresholding.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`, reflecting the module refactoring.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/_basic.py\",\n",
      "            \"function\": \"_set_latest_update\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Replaced direct `zarr.consolidate_metadata(os.path.join(self.group.store.path, self.group.path))` with the new `consolidate_metadata(self.group)` from `bioalpha.compat`, improving maintainability across Zarr versions.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/_basic.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated imports for `get_group_path` and `consolidate_metadata` to point to the new `bioalpha.compat` module.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/_basic.py\",\n",
      "            \"function\": \"_validate_key\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Formatted the long string literal onto multiple lines for improved readability.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/anndata/__init__.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/anndata/__init__.py\",\n",
      "            \"function\": \"SharedAssay.__init__\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed `zarr.Group(public_path, read_only=public_read_only)` to `zarr.open_group(public_path, mode='r')`. This aligns with the Zarr v3 API for read-only group access.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/anndata/__init__.py\",\n",
      "            \"function\": \"SharedAnnData.__init__\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed `zarr.Group(public_path, read_only=public_read_only)` to `zarr.open_group(public_path, mode='r')`. Similar Zarr v3 API compatibility update for read-only access.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/anndata/container.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/dataframe.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Removed direct `os`, `uuid`, and `zarr.attrs` imports. Added `ZarrAttributes` and `open_metadata_attrs` from `bioalpha.compat`, and `get_df_column_length` from `bioalpha.utils`, streamlining dependencies.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/dataframe.py\",\n",
      "            \"function\": \"BackedDataFrame.__len__\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Replaced `len(self.group[self._index_key])` with `get_df_column_length(self.group[self._index_key])`. This provides a more robust way to determine the length of Zarr/H5 dataset columns, especially for categorical data stored with 'codes'.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/dataframe.py\",\n",
      "            \"function\": \"AppObs.z_metadata_group\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Delegated Zarr attribute group creation to `open_metadata_attrs` from `bioalpha.compat`, ensuring compatibility with different Zarr versions for metadata handling.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/__init__.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Changed elements in `__all__` list from unquoted identifiers to quoted strings, which is a stylistic improvement for clarity and consistency.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_base.py\",\n",
      "            \"function\": \"ChunkReader.get_chunk\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Changed `len(self._array)` to `self._array.shape[0]`. This is a more robust way to access the size of the first dimension for array-like objects, improving reliability.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated imports for `get_group_path`, `zarr_copy`, `get_reader`, and `create_dataset` to source them from `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\n",
      "            \"function\": \"_patched_methods\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Modified the AnnData reader retrieval to use `get_reader(_REGISTRY, f, iospec)` from `bioalpha.compat`, ensuring version-aware handling of AnnData readers.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\n",
      "            \"function\": \"write_sparse_collection\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Replaced `group.create_dataset(...)` with `create_dataset(group, ...)` from `bioalpha.compat`, ensuring compatibility for dataset creation across Zarr versions.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\n",
      "            \"function\": \"read_csc_collection\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed `CSCMatrix(zarr.Group(path, read_only=True))` to `CSCMatrix(zarr.open_group(path, mode='r'))`, aligning with Zarr v3 API for read-only group access.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_io_register.py\",\n",
      "            \"function\": \"read_csr_collection\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed `CSCMatrix(zarr.Group(path, read_only=True))` to `CSCMatrix(zarr.open_group(path, mode='r'))`, aligning with Zarr v3 API for read-only group access for CSR collections (note: the code creates a `CSCMatrix` for CSR collection, which might be a copy-paste error or intentional and needs clarification).\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated imports for `get_group_path`, `zarr_copy`, and `create_dataset` from `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\n",
      "            \"function\": \"ActualSpMatrix.set_indptr\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Replaced `self.group.create_dataset(...)` with `create_dataset(self.group, ...)` from `bioalpha.compat`, abstracting Zarr v2/v3 dataset creation.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\n",
      "            \"function\": \"ActualSpMatrix.init_indices\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Replaced `self.group.create_dataset(...)` with `create_dataset(self.group, ...)` from `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\n",
      "            \"function\": \"ActualSpMatrix.set_indices\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed type comparison from `is np.dtype` to `== np.dtype` for `val.dtype`, improving correctness. Also, calls `zarr_copy` from `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\n",
      "            \"function\": \"ActualSpMatrix.init_data\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Replaced `self.group.create_dataset(...)` with `create_dataset(self.group, ...)` from `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\n",
      "            \"function\": \"ActualSpMatrix.set_data\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed type comparison from `is np.dtype` to `== np.dtype` for `val.dtype`, improving correctness. Also, calls `zarr_copy` from `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/matrix/_spmatrix.py\",\n",
      "            \"function\": \"ActualSpMatrix._from_diff_major_axis\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Uses `get_group_path` from `bioalpha.compat` for temporary directory creation, and added parentheses around the conditional `batch.tocsr() if ... else batch.tocsc()` for clarity.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/_base.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/_io_register.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated the import path for `get_group_path` from `bioalpha.utils` to `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/images.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Removed `zarr.attrs` import. Updated `get_group_path` from `bioalpha.utils` to `bioalpha.compat`. Added `create_dataset` and `is_zarr_v3` from `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/images.py\",\n",
      "            \"function\": \"SpatialDataImages.set_channel_histogram\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Replaced `group.create_dataset(...)` with `create_dataset(group, ...)` from `bioalpha.compat`, ensuring Zarr v2/v3 compatibility.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/images.py\",\n",
      "            \"function\": \"SpatialDataImages.generator_writer\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Replaced `group.create_dataset(...)` with `create_dataset(group, ...)` for Zarr v2/v3 compatibility. `dimension_separator` is now passed via `zarr_write_kwargs` (if Zarr v3), indicating more flexible Zarr array creation.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/images.py\",\n",
      "            \"function\": \"SpatialDataImages.generator_writer\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Explicitly casts `level_shape` elements to `int` using `tuple(int(x) for x in ...)` to prevent potential float values from `np.ceil` in shapes.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/images.py\",\n",
      "            \"function\": \"SpatialDataImages._write.register(zarr.Array)\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Added conditional logic for handling `chunk_key_encoding` (Zarr v3) vs `dimension_separator` (Zarr v2) when optimizing Zarr writing, ensuring compatibility with new Zarr features.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Updated imports for `get_group_path` and `write_shapes` to come from `bioalpha.compat`, reflecting the compatibility module.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\n",
      "            \"function\": \"BackedGeoDF.__init__\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed argument name `format` to `element_format` in the `write_shapes` call. This is an API change in `write_shapes` and needs to be consistent with the wrapped/modified function in `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\n",
      "            \"function\": \"BackedGeoDF.geometry.setter\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed argument name `format` to `element_format` in the `write_shapes` call, similar to the `__init__` method.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/store/spatialdata/spatial_df.py\",\n",
      "            \"function\": \"SpatialDataShapes.read\",\n",
      "            \"severity\": \"medium\",\n",
      "            \"comment\": \"Changed `read_elem(self.group[element])` to `_read_shapes(get_group_path(self.group[element]))`. This shifts from using `anndata`'s generic element reader to `spatialdata`'s specific shape reader via the compatibility `get_group_path`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/utils.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"high\",\n",
      "            \"comment\": \"Removed `os`, `Path`, `zarr.storage` imports, and `get_group_path`, `zarr_copy` functions. These have been moved to `bioalpha/compat.py`. This is a significant refactoring and requires verifying all former call sites now import from `bioalpha.compat`.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"src/bioalpha/utils.py\",\n",
      "            \"function\": \"get_df_column_length\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"New utility function to correctly determine the length of a DataFrame column, accounting for AnnData-style categorical storage (which uses a 'codes' array within a group) or direct Zarr/H5 datasets.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"test/spatial/test_imaging.py\",\n",
      "            \"function\": \"(module)\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Imported `create_dataset` from `bioalpha.compat` to use the new compatibility function for creating Zarr datasets in tests.\"\n",
      "        },\n",
      "        {\n",
      "            \"file\": \"test/spatial/test_imaging.py\",\n",
      "            \"function\": \"test_otsu_thresholding\",\n",
      "            \"severity\": \"low\",\n",
      "            \"comment\": \"Replaced `group.create_dataset(...)` with `create_dataset(group, ...)` from `bioalpha.compat` in the test setup, ensuring consistency with the new Zarr compatibility layer.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from src.llm_client.utils import extract_json\n",
    "import json\n",
    "\n",
    "print(json.dumps(extract_json(raw_response), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kudo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
